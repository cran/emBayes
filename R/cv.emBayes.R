#' @importFrom Rcpp sourceCpp
NULL

#' k-folds cross-validation for 'emBayes'
#' 
#' This function performs cross-validation and returns the optimal values of the tuning parameters.
#' @param y a vector of response variable.
#' @param clin a matrix of clinical factors. It has default value NULL.
#' @param X a matrix of genetic factors.
#' @param quant value of quantile.
#' @param t0 a user-supplied sequence of the spike scale \eqn{s_{0}}.
#' @param t1 a user-supplied sequence of the slab scale \eqn{s_{1}}.
#' @param k number of folds for cross-validation.
#' @param func methods to perform variable selection. Two choices are available: "ssLASSO" and "ssQLASSO".
#' @param error cutoff value for determining convergence. The algorithm reaches convergence if the difference in the expected log-likelihood of two iterations is less than the value of error. The default value is 0.01.
#' @param maxiter the maximum number of iterations that is used in the estimation algorithm. The default value is 200.
#' @details
#' When performing cross-validation for emBayes, function cv.emBayes returns two sets of optimal tuning parameters and their corresponding cross-validation error matrices. 
#' The spike scale parameter \eqn{CL.s0} and the slab scale parameter \eqn{CL.s1} are obtained based on the quantile check loss. 
#' The spike scale parameter \eqn{SL.s0} and the slab scale parameter \eqn{SL.s1} are obtained based on the least squares loss. 
#' The spike scale parameter \eqn{SIC.s0} and the slab scale parameter \eqn{SIC.s1} are obtained based on the Schwarz Information Criterion (SIC).
#' Corresponding error matrices \eqn{CL.CV}, \eqn{SL.CV} and \eqn{SIC.CV} can also be obtained from the output.
#' 
#' Schwarz Information Criterion has the following form:
#' \deqn{SIC=\log\sum_{i=1}^nL(y_i-\hat{y_i})+\frac{\log n}{2n}edf}
#' where \eqn{L(\cdot)} is the check loss and \eqn{edf} is the number of close to zero residuals \eqn{(\leq 0.001)}. 
#' 
#' @return A list with components:
#' \item{CL.s0}{the optimal spike scale under check loss.}
#' \item{CL.s1}{the optimal slab scale under check loss.}
#' \item{SL.s0}{the optimal slab scale under least squares loss.}
#' \item{SL.s1}{the optimal slab scale under least squares loss.}
#' \item{SIC.s0}{the optimal slab scale under SIC.}
#' \item{SIC.s1}{the optimal slab scale under SIC.}
#' \item{CL.CV}{cross-validation error matrix under check loss.}
#' \item{SL.CV}{cross-validation error matrix under least squares loss.}
#' \item{SIC.CV}{cross-validation error matrix under SIC.}
#' 
#' @export

cv.emBayes <- function(y,clin=NULL,X,quant,t0,t1,k,func,error=0.01,maxiter=200){
  l1 <- length(t0)
  l2 <- length(t1)
  n <- nrow(X)
  inter <- rep(1,n)
  C <- cbind(inter,clin)
  
  s <- sample(1:n,n,replace=FALSE)
  folds <- cut(s,breaks=k,labels=FALSE)
  
  CV1 <- matrix(0,l1,l2)
  CV2 <- matrix(0,l1,l2)
  CV3 <- matrix(0,l1,l2)
  for(i in 1:l1){
      for(j in 1:l2){
        rob <- rep(0,k)
        nrob <- rep(0,k)
        SIC <- rep(0,k)
        for(f in 1:k){
          k.sub <- which(folds==f)
          EM <- emBayes(y[-k.sub], clin[-k.sub,], X[-k.sub,],quant,t0[i],t1[j],func,error,maxiter)
          beta_k <- EM$beta
          alpha_k <- EM$alpha
          residual <- y[k.sub]-X[k.sub,]%*%beta_k-C[k.sub,]%*%alpha_k
          edf <- sum(abs(residual) <= 1e-03)
          rob[f] <- sum(residual*(quant-(residual < 0))) 
          nrob[f] <- sum(residual^2)
          SIC[f] <- log(rob[f])+log(n)*edf/(2*n)
        }
        CV1[i,j] <- sum(rob)/k
        CV2[i,j] <- sum(nrob)/k
        CV3[i,j] <- sum(SIC)/k
      }
  }

  indices1 <- which(CV1 == min(CV1), arr.ind=TRUE)
  indices2 <- which(CV2 == min(CV2), arr.ind=TRUE)
  indices3 <- which(CV3 == min(CV3), arr.ind=TRUE)
  return(list("CL.s0"=t0[indices1[1]],"CL.s1"=t1[indices1[2]],"CL.CV"=CV1,"SL.s0"=t0[indices2[1]],"SL.s1"=t1[indices2[2]],"SL.CV"=CV2,"SIC.s0"=t0[indices3[1]],"SIC.s1"=t1[indices3[2]],"SIC.CV"=CV3))
}